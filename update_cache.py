#!/usr/bin/env python
"""
update_cache.py
â€“ æœªæ¥æ—¥ã®åœ¨åº«ãƒ»å¹³å‡(æœ€ä½)æ–™é‡‘ã‚’å–å¾—ã—ã¦
  1å: vacancy_price_cache.json / historical_data.json
  2å: vacancy_price_cache_2p.json / historical_data_2p.json
  demand_spike_history.json / last_updated.json ã‚’æ›´æ–°

â€» é‡è¦: ã€å¹³å‡ä¾¡æ ¼ã€ã¯ â€œå„ãƒ›ãƒ†ãƒ«ã®å½“æ—¥æœ€å®‰å€¤(æœ€ä½ä¾¡æ ¼) ã®å¹³å‡â€ ã«çµ±ä¸€
"""

import os
import sys
import json
import time
import calendar
import requests
import datetime as dt
from pathlib import Path
from dateutil.relativedelta import relativedelta


# ============================================================
# Rakuten API credentials (V1 / V2)
#  - ç„¡äº‹æ•…æ–¹é‡ï¼šV2ã®ç’°å¢ƒå¤‰æ•°ãŒæƒã£ã¦ã„ã‚‹æ™‚ã ã‘V2ã‚’ä½¿ã„ã€
#               ç„¡ã‘ã‚Œã°å¾“æ¥ã©ãŠã‚ŠV1ã§å‹•ã‹ã™
# ============================================================
APP_ID_V1 = os.environ.get("RAKUTEN_APP_ID", "").strip()

APP_ID_V2 = os.environ.get("RAKUTEN_APP_ID_V2", "").strip()
ACCESS_KEY_V2 = os.environ.get("RAKUTEN_ACCESS_KEY_V2", "").strip()

# ä»»æ„ï¼šå¼·åˆ¶ãƒ¢ãƒ¼ãƒ‰ï¼ˆauto / v1 / v2ï¼‰ â€»æœªè¨­å®šãªã‚‰ auto
API_MODE = os.environ.get("RAKUTEN_API_MODE", "auto").strip().lower()

USE_V2 = (API_MODE == "v2") or (API_MODE == "auto" and APP_ID_V2 and ACCESS_KEY_V2)

if USE_V2:
    if not APP_ID_V2 or not ACCESS_KEY_V2:
        raise ValueError("âŒ V2ãƒ¢ãƒ¼ãƒ‰ãªã®ã« RAKUTEN_APP_ID_V2 / RAKUTEN_ACCESS_KEY_V2 ãŒæœªè¨­å®šã§ã™ã€‚")
else:
    if not APP_ID_V1:
        raise ValueError("âŒ RAKUTEN_APP_ID ãŒè¨­å®šã•ã‚Œã¦ã„ã¾ã›ã‚“ã€‚GitHub Secrets ã«ç™»éŒ²ã—ã¦ãã ã•ã„ã€‚")

# æ—¢å­˜ã‚³ãƒ¼ãƒ‰äº’æ›ç”¨ï¼šAPP_ID ã¯ã€Œä»Šä½¿ã†æ–¹ã€ã‚’å…¥ã‚Œã¦ãŠã
APP_ID = APP_ID_V2 if USE_V2 else APP_ID_V1

# ã‚¨ãƒ³ãƒ‰ãƒã‚¤ãƒ³ãƒˆï¼ˆV1 / V2ï¼‰
RAKUTEN_API_URL_V1 = "https://app.rakuten.co.jp/services/api/Travel/VacantHotelSearch/20170426"
RAKUTEN_API_URL_V2 = "https://openapi.rakuten.co.jp/engine/api/Travel/VacantHotelSearch/20170426"
RAKUTEN_API_URL = RAKUTEN_API_URL_V2 if USE_V2 else RAKUTEN_API_URL_V1

# V2ã¯ Referer/Origin ãŒå¿…è¦ã«ãªã‚‹ã‚±ãƒ¼ã‚¹ãŒã‚ã‚‹ãŸã‚ã€æ˜ç¤ºã—ã¦ä»˜ã‘ã‚‹ï¼ˆSmokeTestã§æˆåŠŸæ¸ˆã¿ï¼‰
HTTP_REFERER = os.environ.get("RAKUTEN_HTTP_REFERER", "https://mizutanigrandee.github.io/").strip()
HTTP_ORIGIN  = os.environ.get("RAKUTEN_HTTP_ORIGIN",  "https://mizutanigrandee.github.io").strip()

RAKUTEN_HEADERS = {}
if USE_V2:
    # accessKey ã¯ query ã«ã‚‚å…¥ã‚Œã‚‹ï¼ˆV2ã®è¦æ±‚ã«ç¢ºå®Ÿã«åˆã†ï¼‰ + Bearer ã‚‚ä½µç”¨
    RAKUTEN_HEADERS = {
        "Authorization": f"Bearer {ACCESS_KEY_V2}",
        "Referer": HTTP_REFERER,
        "Origin": HTTP_ORIGIN,
        "User-Agent": "vacancy-dashboard/update_cache",
    }

print(f"ğŸ§© Rakuten API mode: {'V2' if USE_V2 else 'V1'}", file=sys.stderr)


# â˜… è‡ªç¤¾ã®æ¥½å¤©æ–½è¨­ç•ªå·ã¯ Secrets å¿…é ˆï¼ˆç›´æ›¸ãã—ãªã„ï¼‰
MY_HOTEL_NO = os.environ.get("RAKUTEN_MY_HOTEL_NO", "")
if not MY_HOTEL_NO or not MY_HOTEL_NO.strip().isdigit():
    raise ValueError("âŒ RAKUTEN_MY_HOTEL_NO ãŒæœªè¨­å®š or ä¸æ­£ã§ã™ã€‚GitHub Secrets ã«æ•°å­—ã®ã¿ã§ç™»éŒ²ã—ã¦ãã ã•ã„ã€‚")
MY_HOTEL_NO = MY_HOTEL_NO.strip()

# ---------- 1å / 2å å‡ºåŠ›ãƒ•ã‚¡ã‚¤ãƒ« ----------
CACHE_FILE_1P          = "vacancy_price_cache.json"
PREV_CACHE_FILE_1P     = "vacancy_price_cache_previous.json"
HISTORICAL_FILE_1P     = "historical_data.json"

CACHE_FILE_2P          = "vacancy_price_cache_2p.json"
PREV_CACHE_FILE_2P     = "vacancy_price_cache_2p_previous.json"
HISTORICAL_FILE_2P     = "historical_data_2p.json"

SPIKE_HISTORY_FILE     = "demand_spike_history.json"
LAST_UPDATED_FILE      = "last_updated.json"   # ãƒ•ãƒ­ãƒ³ãƒˆãŒèª­ã‚€æœ€çµ‚æ›´æ–°ãƒ¡ã‚¿

MAX_PAGES = 3  # å¸‚å ´å´ã®ãƒšãƒ¼ã‚¸èµ°æŸ»ä¸Šé™ï¼ˆãƒ‘ãƒ•ã‚©ãƒ¼ãƒãƒ³ã‚¹é…æ…®ï¼‰


# ============================================================
# 429å¯¾ç­–ï¼ˆã‚¹ãƒ­ãƒƒãƒˆãƒªãƒ³ã‚°ï¼‹ãƒªãƒˆãƒ©ã‚¤ï¼‰
# ============================================================
THROTTLE_SEC = float(os.environ.get("RAKUTEN_THROTTLE_SEC", "0.35"))  # ç´„2.8req/sec
MAX_RETRIES  = int(os.environ.get("RAKUTEN_MAX_RETRIES", "5"))

_session = requests.Session()

def rakuten_get_json(url: str, params: dict, headers: dict = None, timeout: int = 10) -> dict:
    last_err = None
    for attempt in range(MAX_RETRIES):
        try:
            r = _session.get(url, params=params, headers=headers, timeout=timeout)

            if r.status_code == 200:
                if THROTTLE_SEC > 0:
                    time.sleep(THROTTLE_SEC)
                return r.json()

            if r.status_code == 429:
                retry_after = r.headers.get("Retry-After")
                base = int(retry_after) if (retry_after and retry_after.isdigit()) else 2
                wait = min(base * (2 ** attempt), 20)
                print(f"  âš ï¸ 429 Too Many Requests: retry in {wait}s (attempt {attempt+1}/{MAX_RETRIES})", file=sys.stderr)
                time.sleep(wait)
                continue

            if r.status_code in (500, 502, 503, 504):
                wait = min(2 * (2 ** attempt), 20)
                print(f"  âš ï¸ {r.status_code} server error: retry in {wait}s (attempt {attempt+1}/{MAX_RETRIES})", file=sys.stderr)
                time.sleep(wait)
                continue

            last_err = f"HTTP {r.status_code}: {r.text[:200]}"
            break

        except Exception as e:
            last_err = f"exception: {e}"
            wait = min(2 * (2 ** attempt), 20)
            print(f"  âš ï¸ request exception: retry in {wait}s (attempt {attempt+1}/{MAX_RETRIES})", file=sys.stderr)
            time.sleep(wait)

    raise RuntimeError(f"rakuten_get_json failed: {last_err or 'unknown error'}")


# ------------------------------------------------------------
# ä¾¡æ ¼æŠ½å‡ºãƒ˜ãƒ«ãƒ‘ãƒ¼ï¼š1ãƒ›ãƒ†ãƒ«å¡Šã®â€œå½“æ—¥æœ€å®‰å€¤(æœ€ä½ä¾¡æ ¼)â€ã‚’è¿”ã™
# ------------------------------------------------------------
def _extract_hotel_min_price(hotel_obj):
    try:
        blocks = hotel_obj.get("hotel", [])
        if len(blocks) < 2:
            return None
        room_block = blocks[1]  # roomInfo é…åˆ—ãŒå…¥ã£ã¦ã„ã‚‹å´
        min_price = None
        for ri in room_block.get("roomInfo", []):
            dc = ri.get("dailyCharge") or {}
            total = dc.get("total")
            if isinstance(total, (int, float)) and total > 0:
                if (min_price is None) or (total < min_price):
                    min_price = total
        return min_price
    except Exception:
        return None


# ------------------------------------------------------------
# æ¥½å¤©APIï¼šå¸‚å ´ã®åœ¨åº«æ•°ã¨å¹³å‡(æœ€ä½)ä¾¡æ ¼ï¼ˆadultNumå¯å¤‰ï¼‰
# ------------------------------------------------------------
def fetch_market_avg(date: dt.date, adult_num: int) -> dict:
    print(f"ğŸ” market({adult_num}p) {date}", file=sys.stderr)
    hotel_mins = []
    vacancy_total = 0

    for page in range(1, MAX_PAGES + 1):
        params = {
            "applicationId": APP_ID,
            "format": "json",
            "checkinDate":  date.strftime("%Y-%m-%d"),
            "checkoutDate": (date + dt.timedelta(days=1)).strftime("%Y-%m-%d"),
            "adultNum": adult_num,
            "largeClassCode":  "japan",
            "middleClassCode": "osaka",
            "smallClassCode":  "shi",
            "detailClassCode": "D",
            "page": page,
        }

        if USE_V2:
            params["applicationId"] = APP_ID_V2
            params["accessKey"] = ACCESS_KEY_V2
        else:
            params["applicationId"] = APP_ID_V1

        try:
            data = rakuten_get_json(RAKUTEN_API_URL, params=params, headers=RAKUTEN_HEADERS, timeout=10)
        except Exception as e:
            print(f"  âš ï¸ market fetch error {date} p{page}: {e}", file=sys.stderr)
            continue

        if page == 1:
            vacancy_total = data.get("pagingInfo", {}).get("recordCount", 0)

        for h in data.get("hotels", []):
            mp = _extract_hotel_min_price(h)
            if isinstance(mp, (int, float)):
                hotel_mins.append(mp)

    avg_price = round(sum(hotel_mins) / len(hotel_mins), 0) if hotel_mins else 0.0
    print(f"   â†’ market({adult_num}p) avg(min) = {avg_price}  (vacancy={vacancy_total}, hotels={len(hotel_mins)})", file=sys.stderr)
    return {"vacancy": vacancy_total, "avg_price": avg_price}


# ------------------------------------------------------------
# æ¥½å¤©APIï¼šè‡ªç¤¾ãƒ›ãƒ†ãƒ«ã®å½“æ—¥æœ€å®‰å€¤ï¼ˆadultNumå¯å¤‰ï¼‰
# ------------------------------------------------------------
def fetch_my_min_price(date: dt.date, hotel_no: str, adult_num: int) -> float:
    if not hotel_no:
        return 0.0

    params = {
        "applicationId": APP_ID,
        "format": "json",
        "checkinDate":  date.strftime("%Y-%m-%d"),
        "checkoutDate": (date + dt.timedelta(days=1)).strftime("%Y-%m-%d"),
        "adultNum": adult_num,
        "hotelNo": hotel_no,
        "detailClassCode": "D",
        "page": 1,
    }

    if USE_V2:
        params["applicationId"] = APP_ID_V2
        params["accessKey"] = ACCESS_KEY_V2
    else:
        params["applicationId"] = APP_ID_V1

    try:
        data = rakuten_get_json(RAKUTEN_API_URL, params=params, headers=RAKUTEN_HEADERS, timeout=10)
    except Exception as e:
        print(f"  âš ï¸ my fetch error {date} ({adult_num}p): {e}", file=sys.stderr)
        return 0.0

    mins = []
    for h in data.get("hotels", []):
        mp = _extract_hotel_min_price(h)
        if isinstance(mp, (int, float)):
            mins.append(mp)

    my_min = float(min(mins)) if mins else 0.0
    print(f"   â†’ my({adult_num}p) min = {my_min}", file=sys.stderr)
    return my_min


def _load_json_file(path: str) -> dict:
    p = Path(path)
    if not p.exists():
        return {}
    try:
        return json.loads(p.read_text(encoding="utf-8"))
    except Exception:
        return {}


def _save_json_file(path: str, data: dict):
    Path(path).write_text(json.dumps(data, ensure_ascii=False, indent=2), encoding="utf-8")


# ------------------------------------------------------------
# å½“æ—¥ä»¥é™ã®æœªæ¥æ—¥ã‚’æ›´æ–°ï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥ï¼š1å/2åï¼‰
# ------------------------------------------------------------
def update_cache_mode(start_date: dt.date, months: int, adult_num: int, cache_file: str, prev_file: str) -> dict:
    today            = dt.date.today()
    three_months_ago = today - relativedelta(months=3)
    cal              = calendar.Calendar(firstweekday=calendar.SUNDAY)

    cache = _load_json_file(cache_file)
    old_cache = _load_json_file(prev_file)

    # éå»3ã‹æœˆã‚ˆã‚Šå‰ã¯å‰Šé™¤
    cache = {k: v for k, v in cache.items() if _is_date_string(k) and dt.date.fromisoformat(k) >= three_months_ago}

    for m in range(months):
        month_start = (start_date + relativedelta(months=m)).replace(day=1)
        for week in cal.monthdatescalendar(month_start.year, month_start.month):
            for day in week:
                if day.month != month_start.month or day <= today:
                    continue

                iso = day.isoformat()

                market = fetch_market_avg(day, adult_num=adult_num)
                my_p = 0.0
                try:
                    my_p = fetch_my_min_price(day, MY_HOTEL_NO, adult_num=adult_num)
                except Exception as e:
                    print(f"  âš ï¸ my price error {iso} ({adult_num}p): {e}", file=sys.stderr)

                # APIå¤±æ•—æ—¥ã¯ã‚¹ã‚­ãƒƒãƒ—ã—æ—¢å­˜å€¤ä¿æŒï¼ˆ0/0ã¯æ›´æ–°ã—ãªã„ï¼‰
                if market["vacancy"] == 0 and market["avg_price"] == 0.0:
                    print(f"â© skip {iso} ({adult_num}p) (empty)", file=sys.stderr)
                    continue

                prev       = old_cache.get(iso, {})
                last_vac   = prev.get("vacancy",   market["vacancy"])
                last_price = prev.get("avg_price", market["avg_price"])
                vac_diff   = market["vacancy"] - last_vac
                price_diff = market["avg_price"] - last_price

                my_vs_avg_pct = (
                    round((my_p - market["avg_price"]) / market["avg_price"] * 100, 1)
                    if (my_p and market["avg_price"]) else None
                )

                cache[iso] = {
                    "vacancy":        market["vacancy"],
                    "avg_price":      market["avg_price"],
                    "last_vacancy":   last_vac,
                    "last_avg_price": last_price,
                    "vacancy_diff":   vac_diff,
                    "avg_price_diff": price_diff,
                    # è‡ªç¤¾æƒ…å ±ï¼ˆ1å/2åã©ã¡ã‚‰ã‚‚åŒã˜ã‚­ãƒ¼åã§ä¿å­˜ï¼‰
                    "my_price":       my_p if my_p else 0.0,
                    "my_vs_avg_pct":  my_vs_avg_pct,
                }

    _save_json_file(cache_file, cache)
    _save_json_file(prev_file, cache)  # æ¬¡å›æ¯”è¼ƒç”¨ã«â€œä»Šå›å€¤â€ã‚’ä¿å­˜
    print(f"âœ… cache updated: {cache_file}", file=sys.stderr)
    return cache


def _is_date_string(s: str) -> bool:
    try:
        dt.date.fromisoformat(s)
        return True
    except ValueError:
        return False


# ------------------------------------------------------------
# éå»3ã‹æœˆã®ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆå±¥æ­´ï¼ˆãƒ¢ãƒ¼ãƒ‰åˆ¥ï¼‰
# ------------------------------------------------------------
def update_history_mode(cache: dict, historical_file: str):
    today     = dt.date.today()
    today_str = today.isoformat()

    hist_data = _load_json_file(historical_file)

    # æœªæ¥æ—¥ã®ä»Šæ—¥æ™‚ç‚¹ã‚¹ãƒŠãƒƒãƒ—ã‚·ãƒ§ãƒƒãƒˆã‚’è¿½è¨˜
    for iso, v in cache.items():
        if _is_date_string(iso) and dt.date.fromisoformat(iso) >= today:
            hist_data.setdefault(iso, {})
            hist_data[iso][today_str] = {
                "vacancy":   v.get("vacancy", 0),
                "avg_price": v.get("avg_price", 0),
            }

    # å„å¯¾è±¡æ—¥ã®å±¥æ­´ã‚’3ã‹æœˆã«åœ§ç¸®
    for date_key in list(hist_data.keys()):
        if not _is_date_string(date_key):
            del hist_data[date_key]
            continue

        date_dt = dt.date.fromisoformat(date_key)
        limit   = date_dt - relativedelta(months=3)

        for hist_key in list(hist_data[date_key].keys()):
            if not _is_date_string(hist_key):
                del hist_data[date_key][hist_key]
                continue
            hist_dt = dt.date.fromisoformat(hist_key)
            if hist_dt < limit:
                del hist_data[date_key][hist_key]

        if not hist_data[date_key]:
            del hist_data[date_key]

    _save_json_file(historical_file, hist_data)
    print(f"ğŸ“ {historical_file} updated", file=sys.stderr)


# ------------------------------------------------------------
# æ€¥é¨°æ¤œçŸ¥ï¼ˆæ–¹å‘å›ºå®šï¼šå®¢å®¤â†“ Ã— å˜ä¾¡â†‘ï¼‰â€»å½“é¢ã¯1åã®å¸‚å ´å€¤ã§é‹ç”¨
# ------------------------------------------------------------
def detect_demand_spikes(cache_data, price_up_pct=0.05, vac_down_pct=0.05):
    sorted_dates = sorted(cache_data.keys())
    today = dt.date.today()

    results = []
    for d in sorted_dates:
        try:
            stay_dt = dt.date.fromisoformat(d)
        except Exception:
            continue
        if stay_dt < today:
            continue

        rec = cache_data[d]
        last_price = rec.get("last_avg_price", 0)
        last_vac   = rec.get("last_vacancy", 0)
        cur_price  = rec.get("avg_price", 0)
        cur_vac    = rec.get("vacancy", 0)

        if not (last_price and last_vac):
            continue

        price_diff  = cur_price - last_price
        vac_diff    = cur_vac   - last_vac
        price_ratio = (price_diff / last_price) if last_price else 0.0
        vac_ratio   = (vac_diff   / last_vac)   if last_vac   else 0.0

        if (vac_ratio <= -vac_down_pct) and (price_ratio >= price_up_pct):
            results.append({
                "spike_date": d,
                "price": cur_price,
                "last_price": last_price,
                "price_diff": price_diff,
                "price_ratio": round(float(price_ratio), 4),
                "vacancy": cur_vac,
                "last_vac": last_vac,
                "vacancy_diff": vac_diff,
                "vacancy_ratio": round(float(vac_ratio), 4),
            })

    print(f"ğŸ“Š Demand Spikes Detected (priceâ†‘ & vacâ†“): {len(results)} ä»¶", file=sys.stderr)
    return results


def save_demand_spike_history(demand_spikes, history_file=SPIKE_HISTORY_FILE):
    today_dt = dt.date.today()
    today_iso = today_dt.isoformat()

    if os.path.exists(history_file):
        try:
            with open(history_file, "r", encoding="utf-8") as f:
                history = json.load(f)
        except Exception as e:
            print(f"âš ï¸ error loading {history_file}: {e}", file=sys.stderr)
            history = {}
    else:
        history = {}

    history[today_iso] = demand_spikes or []

    limit = (today_dt - dt.timedelta(days=90)).isoformat()
    history = {d: v for d, v in history.items() if d >= limit}

    cleaned = {}
    for up_date, items in history.items():
        new_items = []
        for it in items or []:
            sd = it.get("spike_date")
            try:
                if sd and dt.date.fromisoformat(sd) < today_dt:
                    continue
            except Exception:
                pass

            p_diff = it.get("price_diff", 0)
            v_diff = it.get("vacancy_diff", 0)
            if not (isinstance(p_diff, (int, float)) and isinstance(v_diff, (int, float))):
                continue
            if not (p_diff > 0 and v_diff < 0):
                continue

            new_items.append(it)
        cleaned[up_date] = new_items

    with open(history_file, "w", encoding="utf-8") as f:
        json.dump(cleaned, f, ensure_ascii=False, indent=2)
    print(f"ğŸ“ {history_file} cleaned & updated", file=sys.stderr)


# ------------------------------------------------------------
# æœ€çµ‚æ›´æ–°ãƒ¡ã‚¿ã®æ›¸ãå‡ºã—ï¼ˆJSTï¼‰
# ------------------------------------------------------------
def write_last_updated():
    JST = dt.timezone(dt.timedelta(hours=9))
    now = dt.datetime.now(JST)
    payload = {
        "last_updated_iso": now.isoformat(timespec="seconds"),
        "last_updated_jst": now.strftime("%Y-%m-%d %H:%M:%S JST"),
        "source": "github-actions",
        "git_sha": os.environ.get("GITHUB_SHA", "")[:7],
        "note": "vacancy/price crawl finished",
    }
    try:
        with open(LAST_UPDATED_FILE, "w", encoding="utf-8") as f:
            json.dump(payload, f, ensure_ascii=False, indent=2)
        print(f"ğŸ•’ {LAST_UPDATED_FILE} written: {payload['last_updated_jst']}", file=sys.stderr)
    except Exception as e:
        print(f"âš ï¸ failed to write {LAST_UPDATED_FILE}: {e}", file=sys.stderr)


# ------------------------------------------------------------
# ã‚¨ãƒ³ãƒˆãƒªãƒã‚¤ãƒ³ãƒˆ
# ------------------------------------------------------------
if __name__ == "__main__":
    print("ğŸ“¡ update_cache.py start", file=sys.stderr)

    # 1åï¼ˆå¾“æ¥ï¼‰
    cache_1p = update_cache_mode(
        start_date=dt.date.today(),
        months=9,
        adult_num=1,
        cache_file=CACHE_FILE_1P,
        prev_file=PREV_CACHE_FILE_1P
    )
    update_history_mode(cache_1p, HISTORICAL_FILE_1P)

    # æ€¥é¨°ï¼ˆå½“é¢ã¯1åï¼‰
    demand_spikes = detect_demand_spikes(
        cache_data=cache_1p,
        price_up_pct=0.05,
        vac_down_pct=0.05
    )
    save_demand_spike_history(demand_spikes)

    # 2åï¼ˆæ–°è¦ï¼‰
    cache_2p = update_cache_mode(
        start_date=dt.date.today(),
        months=9,
        adult_num=2,
        cache_file=CACHE_FILE_2P,
        prev_file=PREV_CACHE_FILE_2P
    )
    update_history_mode(cache_2p, HISTORICAL_FILE_2P)

    # æœ€å¾Œã«æ›´æ–°ãƒ¡ã‚¿
    write_last_updated()
    print("âœ¨ all done", file=sys.stderr)
